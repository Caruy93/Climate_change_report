{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import Wrangler as Wr\n",
    "## Wr.extractLines\n",
    "## Wr.multipleReplace\n",
    "## Wr.makeDirFile\n",
    "\n",
    "def importBerkeleyData(file_url):\n",
    "    # Access URL, then request and read temperature data file \n",
    "    with urllib.request.urlopen(file_url) as fhand:\n",
    "        # Decode file data and form a list\n",
    "        file = [line.decode('latin-1').strip() for line in fhand]\n",
    "    \n",
    "    # Extract country name\n",
    "    countryName = re.sub('%','',file[4]).strip()\n",
    "\n",
    "    # Identify Index at which temperature data begins\n",
    "    for c,el in enumerate(file):\n",
    "        if len(el) == 0:\n",
    "            break\n",
    "\n",
    "    # Extract data\n",
    "    lst = Wr.extractLines(file[c+1:])\n",
    "\n",
    "    # Parse main headers and sub headers into respective lists\n",
    "    h1 = Wr.multipleReplace({'%':'','-y':'Y'},file[c-2]).split()\n",
    "    h2 = re.sub('[%\\s\\.]*','',file[c-1]).split(',')\n",
    "\n",
    "    # Consolidate main and sub headers into single header list\n",
    "    h_list = [h1[num//2-1] + h2[num] if num > 1 else h2[num] for num in range(len(h2))]\n",
    "   \n",
    "    # Extract mean 1950-1981 mean air temperature used as center of anomalies\n",
    "    centerMean = float(re.sub('^.+?:','',file[49]).split('+/-')[0].strip())\n",
    "    centerUnc = float(re.sub('^.+?:','',file[49]).split('+/-')[1].strip())\n",
    "    \n",
    "    # Land Percentage\n",
    "    landPercent = float(re.findall('^.+?:\\s(.+)%',file[37])[0].strip())\n",
    "\n",
    "    # Convert list to DataFrame\n",
    "    df = pd.DataFrame(lst, columns = h_list) \n",
    "   \n",
    "    # Add Center mean and uncertainty to df\n",
    "    df['centerMean'] = centerMean\n",
    "    df['centerUnc'] = centerUnc\n",
    "    \n",
    "    # Add Land area percent to df\n",
    "    df['landPercent'] = landPercent\n",
    "   \n",
    "    # Create Datetime Index\n",
    "    df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "    # Set data to numeric\n",
    "    df[h_list[2:]] = df[h_list[2:]].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Add Country Name column\n",
    "    df['Country'] = countryName\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the url\n",
    "list_page = 'http://berkeleyearth.lbl.gov/country-list/'\n",
    "\n",
    "# query the website and return the html to the variable ‘page’\n",
    "page = urllib.request.urlopen(list_page)\n",
    "\n",
    "# parse the html using beautiful soup and store in variable `soup`\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "# Create a list of each country link\n",
    "table = soup.find('table', attrs={'class': 'table table-condensed table-hover'})\n",
    "rows = table.findAll('tr')\n",
    "links = []\n",
    "for tr in rows:\n",
    "    col = tr.findAll('td')\n",
    "    if len(col) > 0:\n",
    "        links.append(col[0].find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFileList = []\n",
    "for countryLink in links:\n",
    "    sCountryLink = urllib.parse.quote(countryLink, safe=':/()')\n",
    "    with urllib.request.urlopen(sCountryLink) as page:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "    for a in soup.findAll('a'):\n",
    "        if a.has_attr('href') and str(a['href']).endswith('TAVG-Trend.txt'):\n",
    "            textFileList.append(a['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([importBerkeleyData(url) for url in textFileList],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory path in Data folder\n",
    "exFileName = 'Temp_countries'\n",
    "fullname = Wr.makeDirFile(exFileName)\n",
    "\n",
    "# Export CSV\n",
    "export_csv = df.to_csv(fullname, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
